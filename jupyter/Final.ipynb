{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>(function($) {    function gasFlame() {var colors = ['#fff', '#99f', '#00f','#f00', '#009', '#f90'];        if (Math.random() > 0.90) {            colors.push(colors[4]);        }        var hv = 0.04,            textShadows = [],            c = 0,            h = 0,            s = 0;        while (c < colors.length - 1) {            s = 2 + Math.round(Math.random() * 2);            while (s--) {                shadow = '0 ' + h + 'em ' + -h + 'em ' + colors[c];                textShadows.push(shadow);                h -= hv;            }            c++;        }        $(this).css({            color: colors[0],            textShadow: textShadows.join(', ')        });    }      $.fn.ignite = function() {        return this.each(function() {            var letters = $(this).text().split('');            $(this).html('<span>' + letters.join('</span><span>') + '</span>');            $spans = $(this).find('span');            setInterval(function() {                $spans.each(gasFlame);            }, 100);        });    };})(jQuery);$(function() {    $('ah1').ignite();});</script><h1><ah1 style=\"font-size: 3em !important;\">Reinforcement Learning!</ah1>\n",
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"show code...\" style=\"font-size:10px; margin: auto;\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reinforcement Learning: it lit... By Alex, Gherardo, Other Alex, and Jimmy\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "js = \"<script>(function($) {    function gasFlame() {var colors = ['#fff', '#99f', '#00f','#f00', '#009', '#f90'];        if (Math.random() > 0.90) {            colors.push(colors[4]);        }        var hv = 0.04,            textShadows = [],            c = 0,            h = 0,            s = 0;        while (c < colors.length - 1) {            s = 2 + Math.round(Math.random() * 2);            while (s--) {                shadow = '0 ' + h + 'em ' + -h + 'em ' + colors[c];                textShadows.push(shadow);                h -= hv;            }            c++;        }        $(this).css({            color: colors[0],            textShadow: textShadows.join(', ')        });    }      $.fn.ignite = function() {        return this.each(function() {            var letters = $(this).text().split('');            $(this).html('<span>' + letters.join('</span><span>') + '</span>');            $spans = $(this).find('span');            setInterval(function() {                $spans.each(gasFlame);            }, 100);        });    };})(jQuery);$(function() {    $('ah1').ignite();});</script><h1><ah1 style=\\\"font-size: 3em !important;\\\">\"\n",
    "js += \"Reinforcement Learning!\"\n",
    "js += \"</ah1>\"\n",
    "js += '''\n",
    "<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"show code...\" style=\"font-size:10px; margin: auto;\"></form>'''\n",
    "display(HTML(js))\n",
    "print(\" Reinforcement Learning: it lit... By Alex, Gherardo, Other Alex, and Jimmy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Project Summary\n",
    "Our goal in this project was to balance an inverted pendulum.\n",
    "<br>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Cart-pendulum.svg/300px-Cart-pendulum.svg.png\" style=\"width:200px; height:200px;\">\n",
    "\n",
    "This problem is most naturally suited to reinforcement learning because there is no previous dataset, but rather a reward function. It's good if the pendulum is pointed up, and it's bad if it's at an angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We originally tried to create a neural network, but that proved overcomplicated for this situation. This led us to attempt Q-learning. Q-Learning is a tabular method of reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfacing with the robot\n",
    "The turtlebot uses ROS(Robot operating system). Then it operates on a publisher subscriber system. Unfortunately, this system can cause lags and loss in communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'rospy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fd3df737d245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrospy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mserial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/dev/ttyACM0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrospy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'goalieBot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manonymous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmovement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrospy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPublisher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/mobile_base/commands/velocity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTwist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtcp_nodelay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'rospy'"
     ]
    }
   ],
   "source": [
    "import rospy\n",
    "import roslib\n",
    "ser = serial.Serial('/dev/ttyACM0', 9600)\n",
    "rospy.init_node('goalieBot', anonymous=True)\n",
    "movement = rospy.Publisher(\"/mobile_base/commands/velocity\", Twist, queue_size=None, tcp_nodelay=True, latch=True)\n",
    "timePerStep = 8\n",
    "r = rospy.Rate(timePerStep)\n",
    "moveCmd = Twist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner class\n",
    "The Learner class is what allows the robot to use reinforcement learning. This class receives the reward and state. It then decides the action which should be taken next in order to maximize the expected reward. Then, it obser updates the q-table.\n",
    "* `num_bins`: the number of discrete values we divide our angle measurements\n",
    "* `num_actions`: the number of discrete values divide our possible movements\n",
    "* `q-matrix`: our q-table (with the shape [`num_bins`, `num_bins`, `num_actions`])\n",
    "* `epsilon`: the probability of making a random action\n",
    "* `gamma`: the importance of the future rewards\n",
    "* `alpha`: learning rate of the robot\n",
    "* `epsilonDecay`: decreases the need of randomness as the model becomes more trained\n",
    "* `state`: a 2D array with one axis representing the previous angle measures and the other representing the current angle measures. Having both measures allowed us to take into account the angular velocity of the inverted pendulum\n",
    "* `actionIndex`: the index in our `q-matrix` of the action that would maximize our expected reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    num_bins = 10\n",
    "    num_actions = 30\n",
    "    def __init__(self):\n",
    "        num_states = 2\n",
    "        self.qmatrix = np.zeros((Learner.num_bins+1, Learner.num_bins+1, Learner.num_actions+1))\n",
    "        self.epsilon = .7\n",
    "        self.gamma = .75\n",
    "        self.alpha = .8\n",
    "        self.epsilonDecay = .99\n",
    "        self.state = [0, 0]\n",
    "        self.actionIndex = 0\n",
    "\n",
    "    def getMove(self, newState, reward):\n",
    "        if random.random() < self.epsilon: # allows for exploration (chooses a random action)\n",
    "            return random.randint(0,Learner.num_actions)\n",
    "        newActionIndex = self.qmatrix[newState[0]][newState[1]].argsort()[-1]\n",
    "        #alpha = 1\n",
    "        self.qmatrix[int(self.state[0])][int(self.state[1])][int(self.actionIndex)] = float(self.qmatrix[self.state[0]][self.state[1]][self.actionIndex]) + \\\n",
    "                                    reward + self.gamma * float(self.qmatrix[int(newState[0])][int(newState[1])][int(newActionIndex)])\n",
    "        self.actionIndex = newActionIndex\n",
    "        self.state = newState\n",
    "        self.epsilon *= self.epsilonDecay\n",
    "        return newActionIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speed = 0\n",
    "reward = 0\n",
    "\n",
    "# Turn a value into its nearest bin\n",
    "def to_bin(value, bins):\n",
    "    return np.digitize(x=[value], bins=bins)[0]\n",
    "\n",
    "# Keep reading from serial until a real number is read.\n",
    "# Necessary because Serial isn't always reliable in its reading.\n",
    "def getAngle():\n",
    "    global speed\n",
    "    a=float('inf')\n",
    "    while a == float('inf'):\n",
    "        try:\n",
    "            a = int(ser.readline().decode().strip())\n",
    "        except:\n",
    "            print(\"Failed reading...Trying again...\")\n",
    "    return a\n",
    "\n",
    "# Execute action in the environment\n",
    "def makeMove(action):\n",
    "    global moveCmd\n",
    "    moveCmd = Twist()\n",
    "    moveCmd.linear.x = action\n",
    "    movement.publish(moveCmd)\n",
    "\n",
    "# Gets a reward - we tried two different methods.\n",
    "# This method turned out to be real trouble.\n",
    "def getReward():\n",
    "    global reward\n",
    "    \n",
    "    # Method 1 - get the highest score that is obtained\n",
    "    # in between each timestep. In case by the time it reads\n",
    "    # the pendulum has already gone up and fallen back down.\n",
    "    # The reward is reset to -1 in each step below\n",
    "    reward = max(reward, 100/getAngle())\n",
    "    \n",
    "    # Method 2 - keep a running average of the past 10 angles.\n",
    "    # Prevents faulty readings from causing catostrophic forgetting.\n",
    "    rewardArr = [0 for x in range(10)]\n",
    "    while True:\n",
    "        del rewardArr[0]\n",
    "        rewardArr.append(1000/(abs(getAngle())+1))\n",
    "        reward = np.sum(rewardArr)/len(rewardArr)\n",
    "        time.sleep(.1)\n",
    "\n",
    "def cart_pole_with_qlearning():\n",
    "    global speed, reward\n",
    "    learner = Learner()\n",
    "    angleBins = np.array(list(np.linspace(-60,70,Learner.num_bins)))\n",
    "    actionBins = np.array(list(np.linspace(-1.5,1.5,Learner.num_actions+1)))\n",
    "    previousAngle = getAngle()\n",
    "\n",
    "    try:\n",
    "        print(\"Trying to load matrix...\", end='')\n",
    "        learner.qmatrix = pickle.load(open(\"qMatrix.pkl\", 'rb'))\n",
    "        print(\"Success!\")\n",
    "    except:\n",
    "        print(\"Failed.\")\n",
    "    # we are not going to differentiate between episode and step because,\n",
    "    # when we train the robot, we let it run without ever stopping it\n",
    "    # we are not going to differentiate between episode and step because,\n",
    "    # when we train the robot, we let it run without ever stopping it\n",
    "    # at each episode (i.e. moment when robot fails)\n",
    "    #\n",
    "    # In Reinforcement terminology: Our failure state is the same as our starting\n",
    "    # state, so we do not need the reset stage that occurs in between each epoch\n",
    "    \n",
    "    thread = Thread(target = getReward) # asynchronously start reward function to allow further precision in it.\n",
    "    thread.start()\n",
    "    for step in range(10000):\n",
    "        currentAngle = getAngle()\n",
    "        state = [to_bin(currentAngle, angleBins), to_bin(previousAngle, angleBins)] # get current angle and put it in bin\n",
    "        print(\"Reward:\", reward)\n",
    "        action = actionBins[learner.getMove(state, reward)] # decide on the action\n",
    "        reward = -1 # necessary for method 1 of the reward function\n",
    "        makeMove(action)\n",
    "        previousAngle = currentAngle\n",
    "        time.sleep(1/timePerStep)\n",
    "        #Save the Q-Matrix in between epochs.\n",
    "        if step%5 == 0:\n",
    "            print(\"Saving the q-matrix\")\n",
    "            pickle.dump(learner.qmatrix, open(\"qMatrix.pkl\", 'wb'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cart_pole_with_qlearning()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
